{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4febdd1bdd910197937d794d7fbd716",
     "grade": false,
     "grade_id": "cell-570cf80ae1b2c48e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Actividad 1: Pipeline de procesamiento de datos con HDFS y Spark\n",
    "\n",
    "## Actividad del Proyecto (actividad grupal)\n",
    "\n",
    "Esta actividad está asociada al Proyecto transversal del título y para su desarrollo, tendrás que utilizar obligatoriamente el siguiente recurso:\n",
    "* Dataset flights del Catálogo de Datos del proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escribe aquí los nombres de los integrantes del grupo:\n",
    "\n",
    "## Recuerda borrar siempre las líneas que dicen `raise NotImplementedError`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a238d899b5a6e8c414330ade880233f",
     "grade": false,
     "grade_id": "cell-f4c598b6fd61ee12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Lee con detenimiento cada ejercicio. Las variables utilizadas para almacenar las soluciones, al igual que las nuevas columnas creadas, deben llamarse **exactamente** como indica el ejercicio, o de lo contrario los tests fallarán y el ejercicio no puntuará. Debe reemplazarse el valor `None` al que están inicializadas por el código necesario para resolver el ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db4faded3e277c6e6253d9a9ba99f2d9",
     "grade": false,
     "grade_id": "cell-42368b0202b6ce77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Leemos el fichero flights.csv que hemos subido a Google Cloud Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "219cb0eba4e188af1ef349ee35d7d334",
     "grade": false,
     "grade_id": "cell-3202a483f423590a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Aunque para las capturas de pantalla se pide subir el fichero a HDFS, el resto de la actividad puede hacerse leyendo el mismo fichero que hemos subido al bucket de Google Cloud Storage.\n",
    "\n",
    "Indicamos que contiene encabezados (nombres de columnas) y que intente inferir el esquema, aunque después comprobaremos si lo\n",
    "ha inferido correctamente o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50a7e93a1c2653ab52ccfbaf70c0edbb",
     "grade": false,
     "grade_id": "lectura-fichero",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: hdfs://dataproc-unir-hagr27-m/alejandro_gerena/flights.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33014/3364187418.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruta_gcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: hdfs://dataproc-unir-hagr27-m/alejandro_gerena/flights.csv"
     ]
    }
   ],
   "source": [
    "# Ruta del fichero flights.csv (puedes usar HDFS o GCS)\n",
    "ruta_gcs = \"hdfs:///alejandro_gerena/flights.csv\"\n",
    "\n",
    "# Leer el fichero CSV con Spark\n",
    "flightsDF = (\n",
    "    spark.read \n",
    "    .option(\"header\", \"true\") \n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(ruta_gcs)\n",
    ")\n",
    "\n",
    "# Mostrar esquema y primeras filas para verificar\n",
    "flightsDF.printSchema()\n",
    "flightsDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimimos el esquema para comprobar qué tipo de dato ha inferido en cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos el número de filas que tiene el DataFrame para hacernos una idea de su tamaño:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tenemos 162049 filas. Si imprimimos por pantalla las 5 primeras filas, veremos qué tipos parecen tener y en qué columnas no coincide el tipo que podríamos esperar con el tipo que ha inferido Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La causa del problema es que en muchas columnas existe un valor faltante llamado \"NA\". Spark no reconoce ese valor como\n",
    "*no disponible* ni nada similar, sino que lo considera como un string de texto normal, y por tanto, asigna a toda la columna\n",
    "el tipo de dato string (cadena de caracteres). Concretamente, las siguientes columnas deberían ser de tipo entero pero Spark\n",
    "las muestra como string:\n",
    "<ul>\n",
    " <li>dep_time: string (nullable = true)\n",
    " <li>dep_delay: string (nullable = true)\n",
    " <li>arr_time: string (nullable = true)\n",
    " <li>arr_delay: string (nullable = true)\n",
    " <li>air_time: string (nullable = true)\n",
    " <li>hour: string (nullable = true)\n",
    " <li>minute: string (nullable = true)    \n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a averiguar cuántas filas tienen el valor \"NA\" (como string) en la columna dep_time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "cuantos_NA = (\n",
    "    flightsDF\n",
    "    .where(F.col(\"dep_time\") == \"NA\")\n",
    "    .count()\n",
    ")\n",
    "cuantos_NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por tanto, hay 857 filas que no tienen un dato válido en esa columna. Hay distintas maneras de trabajar con los valores faltantes, como por ejemplo imputarlos (reemplazarlos por un valor generado por nosotros según cierta lógica, por ejemplo la media de esa columna, etc). Lo más sencillo es quitar toda la fila, aunque esto depende de si nos lo podemos permitir en base\n",
    "a la cantidad de datos que tenemos. En nuestro caso, como tenemos un número considerable de filas, vamos a quitar todas las filas donde hay un NA en cualquiera de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_limpiar = [\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\", \"air_time\", \"hour\", \"minute\"]\n",
    "\n",
    "flightsLimpiado = flightsDF\n",
    "for nombreColumna in columnas_limpiar:  # para cada columna, nos quedamos con las filas que no tienen NA en esa columna\n",
    "    flightsLimpiado = flightsLimpiado.where(F.col(nombreColumna) != \"NA\")\n",
    "\n",
    "flightsLimpiado.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si ahora mostramos el número de filas que tiene el DataFrame `flightsLimpiado` tras eliminar todas esas filas, vemos que ha disminuido ligeramente\n",
    "pero sigue siendo un número considerable como para realizar analítica y sacar conclusiones sobre estos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsLimpiado.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos eliminado los NA, vamos a convertir a tipo entero cada una de esas columnas que eran de tipo string. \n",
    "Ahora no debe haber problema ya que todas las cadenas de texto contienen dentro un número que puede ser convertido de texto a número. Vamos también a convertir la columna `arr_delay` de tipo entero a número real, necesario para los pasos posteriores donde ajustaremos un modelo predictivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "flightsConvertido = flightsLimpiado\n",
    "\n",
    "for c in columnas_limpiar:\n",
    "    # método que crea una columna o reemplaza una existente\n",
    "    flightsConvertido = flightsConvertido.withColumn(c, F.col(c).cast(IntegerType())) \n",
    "\n",
    "flightsConvertido = flightsConvertido.withColumn(\"arr_delay\", F.col(\"arr_delay\").cast(DoubleType()))\n",
    "flightsConvertido.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsConvertido.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a volver a mostrar las 5 primeras filas del DataFrame limpio. Aparentemente son iguales a las que ya teníamos, pero ahora\n",
    "Spark sí está tratando como enteros las columnas que deberían serlo, y si queremos podemos hacer operaciones aritméticas\n",
    "con ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsConvertido.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flightsConvertido.groupBy(\"carrier\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "482e3795c3962a4780c282594741996b",
     "grade": false,
     "grade_id": "cell-c0cfdd1db1edaa7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 1 (3 puntos) \n",
    "### (sólo puntúa 3 puntos si pasa la celda de autoevaluación sin errores, o 0 puntos si hay algún error. No hay puntaje intermedio)\n",
    "\n",
    "Partiendo del DataFrame `flightsConvertido` que ya tiene los tipos correctos en las columnas, se pide **encadenar transformaciones sin crear ninguna variable intermedia**, de manera que se vayan creando las siguientes columnas:\n",
    "\n",
    "* Una nueva columna llamada `flight_date` con la fecha del vuelo como objeto fecha. Utiliza para ello la función `F.make_date` aplicada a los argumentos (columnas) `year`, `month` y `day` tal como indica la documentación de `make_date` disponible [aquí](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.make_date.html).\n",
    "* Encadenado con la transformación anterior, crear una nueva columna llamada `day_of_week` con el día de la semana correspondiente al vuelo, utilizando la función `F.dayofweek` cuya documentación puedes ver [aquí](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.dayofweek.html). Debes pasarle como argumento la columna `flight_date` creada en el apartado anterior.\n",
    "* Encadenado con la transformación anterior, agrupar para obtener **tantas filas como aeropuertos de destino existan, y tantas columnas como días de la semana** hay en el DF original. \n",
    "  * Cada celda del DF resultante debe contener el *conteo* del número de vuelos que existen para ese destino en ese día de la semana.\n",
    "  * El DF resultante de la agrupación seguida de pivot tendrá 8 columnas, que serán el destino, y los 7 días de la semana, llamados del 1 al 7.\n",
    "* Encadenado con la transformación anterior, rellenar los valores nulos con un 0, puesto que todos los valores representan conteos, y los nulos han sido provocados porque ciertas combinaciones de (origen, destino, día de la semana) no existen en los datos (no hay vuelos para ciertas rutas en ciertos días de la semana). Por eso, tiene sentido sustituir los nulos por 0, que es el conteo para esa combinación no encontrada.\n",
    "* Encadenado con la transformación anterior, renombrar las columnas de los días de la semana para que pasen a llamarse Domingo, Lunes, ..., Sabado (sin tildes). Puedes hacerlo encadenando `withColumnRenamed` varias veces, o bien con `selectExpr` utilizando `as` en cada argumento de tipo string. Consulta la documentación de `selectExpr` [aquí](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.selectExpr.html).\n",
    "  * Si utilizas selectExpr, entonces debes referirte a las columnas como ``\"`1`\"`` (con acento grave) y no como `\"1\"` ya que, al tener por nombre un número entero, hay que referirse a ellas de ese modo. Si utilizas withColumnRenamed, no es necesario y puedes referirte a ellas como `\"1\"`.\n",
    "  * Si lo deseas, puedes recategorizar la columna day_of_week, justo después de crearla, utilizando para ello otra llamada a `withColumn` para reemplazar dicha columna, usando `F.when` dentro de ella. De este modo, al llegar al groupBy(...).pivot(...) ya tendrá los nombres de columna correctos y no será necesario renombrar columnas.\n",
    "* Encadenado con la transformación anterior, añadir una nueva columna llamada `n_vuelos` que contenga, en cada fila, la suma de las columnas de los 7 días de la semana. Esto representa el número de vuelos totales para ese destino, independientemente del día del vuelo.\n",
    "* Encadenado con la transformación anterior, generar un nuevo DF que esté ordenado por la columna `n_vuelos` *descendentemente*.\n",
    "* El DF resultante de todas estas transformaciones, que deben encadenarse sin utilizar ninguna otra variable, debe quedar guardado en la variable `conteos_df`.\n",
    "* No está permitido utilizar construcciones de la forma df = df.algo(). **Sólo puede utilizarse una vez el operador de asignación (el signo `=`).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7ea4e8f96e75f00e8c2ef6c0668f500",
     "grade": false,
     "grade_id": "ejercicio-1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "conteos_df = (\n",
    "    flightsConvertido\n",
    "    .withColumn(\"flight_date\", F.make_date(F.col(\"year\"), F.col(\"month\"), F.col(\"day\")))\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(F.col(\"flight_date\")))\n",
    "    .groupBy(\"dest\")\n",
    "    .pivot(\"day_of_week\")\n",
    "    .count()\n",
    "    .fillna(0)\n",
    "    .withColumnRenamed(\"1\", \"Domingo\")\n",
    "    .withColumnRenamed(\"2\", \"Lunes\")\n",
    "    .withColumnRenamed(\"3\", \"Martes\")\n",
    "    .withColumnRenamed(\"4\", \"Miercoles\")\n",
    "    .withColumnRenamed(\"5\", \"Jueves\")\n",
    "    .withColumnRenamed(\"6\", \"Viernes\")\n",
    "    .withColumnRenamed(\"7\", \"Sabado\")\n",
    "    .withColumn(\"n_vuelos\", \n",
    "                F.col(\"Domingo\") + F.col(\"Lunes\") + F.col(\"Martes\") + \n",
    "                F.col(\"Miercoles\") + F.col(\"Jueves\") + F.col(\"Viernes\") + F.col(\"Sabado\"))\n",
    "    .orderBy(F.col(\"n_vuelos\").desc())\n",
    ")\n",
    "\n",
    "# Verificar resultado\n",
    "conteos_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e63cd0d4d85442a33c40bb3d6b9cd16",
     "grade": true,
     "grade_id": "ejercicio-1-test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "c = conteos_df.columns\n",
    "assert(len(c) == 9)  # debe tener 9 columnas que son dest, los días de la semana, y n_vuelos (no necesariamente en ese orden)\n",
    "assert(all([x in c for x in [\"dest\", \"n_vuelos\", \"Domingo\", \"Lunes\", \"Martes\", \"Miercoles\", \"Jueves\", \"Viernes\", \"Sabado\"]]))  # Comprobar nombres de columnas\n",
    "cnt_list = conteos_df.take(100)\n",
    "assert(cnt_list[0].dest == \"SFO\" and cnt_list[0].Viernes == 1910)\n",
    "assert(cnt_list[1].dest == \"LAX\" and cnt_list[1].Lunes == 1550)\n",
    "assert(cnt_list[2].dest == \"DEN\" and cnt_list[2].n_vuelos == 9433)\n",
    "assert(cnt_list[-1].Domingo == 0 and cnt_list[-1].n_vuelos == 2)  # la ruta con menos vuelos de todos. Nos aseguramos de que se hayan rellenado los nulos con 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "003daa21c5598342ea0689e753b37f86",
     "grade": false,
     "grade_id": "cell-2b5f0dea18728fcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 2 (3 puntos)\n",
    "### (sólo puntúa 3 puntos si pasa la celda de autoevaluación sin errores, o 0 puntos si hay algún error. No hay puntaje intermedio)\n",
    "\n",
    "Partiendo de nuevo de `flightsConvertido`, se pide crear una función `retraso_medio_periodo`, que reciba como argumento un DF en el podemos asumir que existen las columnas `dep_time` y `arr_delay` y haga lo siguiente:\n",
    "\n",
    "* Añada una nueva columna `periodo_dia` para recategorizar `dep_time` con los siguientes valores sin tildes (puedes hacerlo con withColumn y F.when):\n",
    "  * `\"maniana\"` cuando la hora de salida del vuelo esté entre las 7 de la mañana no incluida (expresado como 700 en dep_time) y las 12 de la mañana incluida (expresado como 1200)\n",
    "  * `\"mediodia\"` cuando la hora de salida del vuelo esté entre las 12 de la mañana no incluida (expresado como 1200) y las 5 de la tarde incluida (expresado como 1700)\n",
    "  * `\"tarde\"` cuando la hora de salida del vuelo esté entre las 5 de la tarde no incluida (expresado como 1700) y las 9 de la noche incluida (expresado como 2100)\n",
    "  * `\"noche\"` cuando la hora de salida del vuelo esté entre las 9 de la noche no incluida (expresado como 2100) y las 7 de la mañana incluida (expresado como 700)\n",
    "  * PISTAS:\n",
    "    * Recuerda que las condiciones compuestas con columnas booleanas en Spark requieren obligatoriamente utilizar paréntesis envolviendo a cada condición simple.\n",
    "    * Recuerda también utilizar F.lit(...) para indicar el valor (constante) de la nueva columna para cada una de estas condiciones.\n",
    "    * Cuidado con la noche: la condición es que la hora sea mayor que 2100 **o bien** (condición `|`) menor o igual que 700. En el resto de condiciones, necesitamos `&`.\n",
    "\n",
    "* Encadenado con la transformación anterior, sólo para los vuelos que llegan con **retraso estrictamente positivo (>0)**, calcular el retraso medio a la llegada de dichos vuelos **para cada aeropuerto de destino y cada hora del día**, desplegando las franjas horarias como columnas independientes. \n",
    "  * El DF calculado debe tener tantas columnas como franjas horarias más la columna del aeropuerto de destino (es decir, 5 columnas en total)\n",
    "\n",
    "* Encadenado con la transformación anterior, tras haber hecho la agrupación y agregación, añadir una columna constante (con `F.lit`) llamada `alumno` que contenga el primer apellido de cada miembro del grupo (ejemplo: si el grupo lo forman Pablo García, Francisco Pérez y Antonio Martín, el valor de la columna debe ser `García_Pérez_Martín`).\n",
    "* Encadenado con la transformación anterior, ordenar alfabéticamente el DF resultante por aeropuerto de destino, ascendentemente. El resultado de la ordenación será un nuevo DF que debe ser devuelto como resultado de la función (recuerda que es imposible modificar el DF original)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cefef7decefd27666b4b92bdcea8a45e",
     "grade": false,
     "grade_id": "ejercicio-2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def retraso_medio_periodo(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calcula el retraso medio a la llegada por aeropuerto de destino y periodo del día,\n",
    "    solo para vuelos con retraso positivo.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"periodo_dia\", \n",
    "                    F.when((F.col(\"dep_time\") > 700) & (F.col(\"dep_time\") <= 1200), F.lit(\"maniana\"))\n",
    "                     .when((F.col(\"dep_time\") > 1200) & (F.col(\"dep_time\") <= 1700), F.lit(\"mediodia\"))\n",
    "                     .when((F.col(\"dep_time\") > 1700) & (F.col(\"dep_time\") <= 2100), F.lit(\"tarde\"))\n",
    "                     .when((F.col(\"dep_time\") > 2100) | (F.col(\"dep_time\") <= 700), F.lit(\"noche\"))\n",
    "                     .otherwise(F.lit(None))\n",
    "        )\n",
    "        .filter(F.col(\"arr_delay\") > 0)\n",
    "        .groupBy(\"dest\")\n",
    "        .pivot(\"periodo_dia\")\n",
    "        .agg(F.avg(\"arr_delay\"))\n",
    "        .withColumn(\"alumno\", F.lit(\"Granados\"))\n",
    "        .orderBy(F.col(\"dest\").asc())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4309c9fd2d3c277d4fb49ab33a609e3f",
     "grade": true,
     "grade_id": "ejercicio-2-tests",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "retraso_medio_df = retraso_medio_periodo(flightsConvertido)\n",
    "lista = retraso_medio_df.take(3)\n",
    "c = retraso_medio_df.columns\n",
    "assert(len(c) == 6)  #  el DF resultante debe tener 6 columnas\n",
    "assert(all([x in c for x in [\"dest\", \"maniana\", \"mediodia\", \"tarde\", \"noche\", \"alumno\"]]))  # Comprobamos los nombres de columnas\n",
    "assert(lista[0].dest == \"ABQ\" and round(lista[0].maniana, 2) == 12.91 and round(lista[0].noche, 2) == 8.33)\n",
    "assert(lista[1].dest == \"ANC\" and round(lista[1].mediodia, 2) == 16.88 and round(lista[1].noche, 2) == 20.70)\n",
    "assert(lista[2].dest == \"ATL\" and round(lista[2].tarde, 2) == 375.0 and round(lista[2].noche, 2) == 18.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora invocamos a nuestra función `retrasoMedio` pasándole como argumento `flightsConvertido`. ¿Cuáles son los tres aeropuertos con mayor retraso medio? ¿Cuáles son sus retrasos medios en minutos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa9ed55cd86eb0958e150d6a918db1af",
     "grade": false,
     "grade_id": "cell-e577747d4427e32b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ejercicio 3\n",
    "\n",
    "Ajustar un modelo de DecisionTree de Spark para predecir si un vuelo vendrá o no con retraso (problema de clasificación binaria), utilizando como variables predictoras el mes, el día del mes, la hora de partida `dep_time`, la hora de llegada `arr_time`, el tipo de avión (`carrier`), la distancia y el tiempo que permanece en el aire. Para ello, sigue los siguientes pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd5d9c35150d9184f07b369c29a44789",
     "grade": false,
     "grade_id": "cell-e577747d4427e32a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notemos que en estos datos hay variables numéricas y variables categóricas que ahora mismo están tipadas como numéricas, como por ejemplo el mes del año (`month`), que es en realidad categórica. Debemos indicar a Spark cuáles son categóricas e indexarlas. Para ello se pide: \n",
    "\n",
    "* **(1 punto)** Crear un `StringIndexer` llamado `indexerMonth` y otro llamado `indexerCarrier` sobre las variables categóricas `month` y `carrier` (tipo de avión). El nombre de las columnas indexadas que se crearán debe ser, respectivamente, `monthIndexed` y `carrierIndexed`. Asegúrate de que, cuando necesite codificar una categoría que no existía cuando se entrenó, esta pieza no lance un error sino que le asigne el primer valor que esté libre (argumento `handleInvalid`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb039584cd14e6bc3434e5be930341e6",
     "grade": false,
     "grade_id": "string-indexer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Crear StringIndexer para month\n",
    "indexerMonth = StringIndexer(\n",
    "    inputCol=\"month\",\n",
    "    outputCol=\"monthIndexed\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Crear StringIndexer para carrier\n",
    "indexerCarrier = StringIndexer(\n",
    "    inputCol=\"carrier\",\n",
    "    outputCol=\"carrierIndexed\",\n",
    "    handleInvalid=\"keep\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb2fa0df13e5dcf2121097b75e7c5dfe",
     "grade": true,
     "grade_id": "string-indexer-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(isinstance(indexerMonth, StringIndexer))\n",
    "assert(isinstance(indexerCarrier, StringIndexer))\n",
    "assert(indexerMonth.getInputCol() == \"month\" and indexerMonth.getOutputCol() == \"monthIndexed\" and indexerMonth.getHandleInvalid() == \"keep\")\n",
    "assert(indexerCarrier.getInputCol() == \"carrier\" and indexerCarrier.getOutputCol() == \"carrierIndexed\" and indexerCarrier.getHandleInvalid() == \"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6a7e3a800a782c19c16b753ef9cc6f2",
     "grade": false,
     "grade_id": "cell-e577747d4427e323",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Recordemos también que Spark requiere que todas las variables estén en una única columna de tipo vector, por lo que después de indexar estas dos variables, tendremos que fusionar en una columna de tipo vector todas ellas, utilizando un `VectorAssembler`. Se pide:\n",
    "\n",
    "* **(1 punto)** Crear en una variable llamada `vectorAssembler` un `VectorAssembler` que reciba como entrada una lista de todas las variables de entrada (y que no debe incluir `arr_delay`) que serán las que formarán parte del modelo. Crear primero esta lista de variables (lista de strings) en la variable `columnas_ensamblar` y pasar dicha variable como argumento al crear el `VectorAssembler`. Como es lógico, en el caso de las columnas `month` y `carrier`, no usaremos las variables originales sino las indexadas en el apartado anterior. La columna de tipo vector creada con las características ensambladas debe llamarse `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44623471d64d0605fcbcd4bbcc3c2a0d",
     "grade": false,
     "grade_id": "vector-assembler",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Lista de columnas a ensamblar (variables predictoras)\n",
    "columnas_ensamblar = [\n",
    "    \"monthIndexed\",      # month indexado (categórico)\n",
    "    \"day\",               # día del mes (numérico)\n",
    "    \"dep_time\",          # hora de partida (numérico)\n",
    "    \"arr_time\",          # hora de llegada (numérico)\n",
    "    \"carrierIndexed\",    # carrier indexado (categórico)\n",
    "    \"distance\",          # distancia (numérico)\n",
    "    \"air_time\"           # tiempo en el aire (numérico)\n",
    "]\n",
    "\n",
    "# Crear VectorAssembler\n",
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=columnas_ensamblar,\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d17b2fa1d8a4bd02b89952429ba1552",
     "grade": true,
     "grade_id": "vector-assembler-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(isinstance(vectorAssembler, VectorAssembler))\n",
    "assert(vectorAssembler.getOutputCol() == \"features\")\n",
    "input_cols = vectorAssembler.getInputCols()\n",
    "assert(len(input_cols) == 7)\n",
    "assert(\"arr_delay\" not in input_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29cd94afed88265b627f01ac03361076",
     "grade": false,
     "grade_id": "cell-e577747d4427e32dsdf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finalmente, vemos que la columna `arr_delay` es continua, y no binaria como requiere un problema de clasificación con dos clases. Vamos a convertirla en binaria. Para ello se pide:\n",
    "\n",
    "* **(0.5 puntos)** Utilizar un binarizador de Spark, fijando a 15 el umbral, y guardarlo en la variable `delayBinarizer`. Consideramos retrasado un vuelo que ha llegado con más de 15 minutos de retraso, y no retrasado en caso contrario. La nueva columna creada con la variable binaria debe llamarse `arr_delay_binary` y debe ser interpretada como la columna target para nuestro algoritmo. Por ese motivo, esta columna **no** se incluyó en el apartado anterior entre las columnas que se ensamblan para formar las features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f33c493a3c3dd83fb6a39a7acefca5c",
     "grade": false,
     "grade_id": "binarizer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "# Crear Binarizer para arr_delay\n",
    "delayBinarizer = Binarizer(\n",
    "    threshold=15.0,\n",
    "    inputCol=\"arr_delay\",\n",
    "    outputCol=\"arr_delay_binary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63866fd5322ae78c26612ef7e1ccdeda",
     "grade": true,
     "grade_id": "binarizer-tests",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(isinstance(delayBinarizer, Binarizer))\n",
    "assert(delayBinarizer.getThreshold() == 15)\n",
    "assert(delayBinarizer.getInputCol() == \"arr_delay\")\n",
    "assert(delayBinarizer.getOutputCol() == \"arr_delay_binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7dbd49d1ba9c1889183ea9d4ef4b1c7",
     "grade": false,
     "grade_id": "cell-25a7793978ee7d05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(0.5 puntos)** Por último, crearemos el modelo de clasificación.\n",
    "\n",
    "* Crear en una variable `decisionTree` un árbol de clasificación de Spark (`DecisionTreeClassifier` del paquete `pyspark.ml.classification`)\n",
    "* Indicar como columna de entrada la nueva columna creada por el `VectorAssembler` creado en un apartado anterior.\n",
    "* Indicar como columna objetivo (target) la nueva columna creada por el `Binarizer` del apartado anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e785136d6d06691c003ff9542027e03d",
     "grade": false,
     "grade_id": "decision-tree",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Crear DecisionTreeClassifier\n",
    "decisionTree = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"arr_delay_binary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6551bea8cb96c1de22e91028295a3f6c",
     "grade": true,
     "grade_id": "decision-tree-tests",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert(isinstance(decisionTree, DecisionTreeClassifier))\n",
    "assert(decisionTree.getFeaturesCol() == \"features\")\n",
    "assert(decisionTree.getLabelCol() == \"arr_delay_binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "008efada185c29e19ebbe2b4a82216fd",
     "grade": false,
     "grade_id": "cell-e577747d4427e32d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(1 punto)** Ahora vamos a encapsular todas las fases en un sólo pipeline y procederemos a entrenarlo. Se pide:\n",
    "\n",
    "* Crear en una variable llamada `pipeline` un objeto `Pipeline` de Spark con las etapas anteriores en el orden adecuado para poder entrenar un modelo. \n",
    "\n",
    "* Entrenarlo invocando sobre ella al método `fit` y guardar el pipeline entrenado devuelto por dicho método en una variable llamada `pipelineModel`. \n",
    "\n",
    "* Aplicar el pipeline entrenado para transformar (predecir) el DataFrame `flightsConvertido`, guardando las predicciones devueltas en la variable `flightsPredictions` que será un DataFrame. Nótese que estamos prediciendo los propios datos de entrenamiento y que, por simplicidad, no habíamos hecho (aunque habría sido lo correcto) ninguna división de nuestros datos originales en subconjuntos distintos de entrenamiento y test antes de entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edbdb627305d03efa41a88426330e160",
     "grade": false,
     "grade_id": "pipeline",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Crear Pipeline con todas las etapas en orden\n",
    "pipeline = Pipeline(stages=[\n",
    "    indexerMonth,       # 1. Indexar month → monthIndexed\n",
    "    indexerCarrier,     # 2. Indexar carrier → carrierIndexed\n",
    "    delayBinarizer,     # 3. Binarizar arr_delay → arr_delay_binary\n",
    "    vectorAssembler,    # 4. Ensamblar features → features\n",
    "    decisionTree        # 5. Entrenar DecisionTreeClassifier\n",
    "])\n",
    "\n",
    "# Entrenar el pipeline\n",
    "pipelineModel = pipeline.fit(flightsConvertido)\n",
    "\n",
    "# Aplicar el pipeline entrenado para hacer predicciones\n",
    "flightsPredictions = pipelineModel.transform(flightsConvertido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "befe2447f17ba2d8174aee074901c82a",
     "grade": true,
     "grade_id": "pipeline-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "assert(isinstance(pipeline, Pipeline))\n",
    "assert(len(pipeline.getStages()) == 5)\n",
    "assert(isinstance(pipelineModel, PipelineModel))\n",
    "assert(\"probability\" in flightsPredictions.columns)\n",
    "assert(\"prediction\" in flightsPredictions.columns)\n",
    "assert(\"rawPrediction\" in flightsPredictions.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c531953abf18cfb3b67571ddde7a57d",
     "grade": false,
     "grade_id": "cell-61156fe5938763f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Vamos a mostrar la matriz de confusión (este apartado no es evaluable). Agrupamos por la variable que tiene la clase verdadera y la que tiene la clase predicha, para ver en cuántos casos coinciden y en cuántos difieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75f98ae39b827e75c3f0b4b2aaa6b0db",
     "grade": false,
     "grade_id": "cell-896752beb71cb455",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "flightsPredictions.groupBy(\"arr_delay_binary\", \"prediction\").count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}