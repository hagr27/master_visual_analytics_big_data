{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319f23f8",
   "metadata": {},
   "source": [
    "# **Actividad 1: Pipeline de procesamiento de datos con HDFS y Spark - Grupal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b393fbef",
   "metadata": {},
   "source": [
    "## **1. Contenido de la actividad**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc725e",
   "metadata": {},
   "source": [
    "La actividad tiene **3 partes** y **2 entregables**:\n",
    "\n",
    "- **Parte 1 – HDFS**\n",
    "    - Trabajo en terminal Linux + comandos HDFS + captura de pantalla.\n",
    "- **Parte 2 – Spark con JupyterLab**\n",
    "    - Notebook `actividad_1.ipynb` trabajando sobre el dataset `flights`.\n",
    "- **Parte 3 – Spark MLlib**\n",
    "    - Continuación en el mismo notebook, siguiendo las instrucciones del propio notebook.\n",
    "- **Entregables:**\n",
    "    1. Notebook `actividad_1.ipynb` completo y funcional.\n",
    "    2. Documento de **máx. 1 página** con capturas de los comandos de HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dade1b6",
   "metadata": {},
   "source": [
    "## **2. Desarrollo de la actividad**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6c0df",
   "metadata": {},
   "source": [
    "### **2.1. Preparación del entorno (previo a las 3 partes)**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc9d9c7",
   "metadata": {},
   "source": [
    "**Desplegar Infraestructura**\n",
    "\n",
    "Para preparar el ambiente, debe dirigirse al proyecto de **infraestructura**, donde se ha creado un proceso en **Terraform** para aprovisionar la infraestructura mediante **IaC** (*Infrastructure as Code*).\n",
    "\n",
    "1. Este proyecto se encuentra en el repositorio, en la ubicación:  \n",
    "  [**Proyecto Terraform**](../../../workshops/01.block/01.infrastructure)\n",
    "\n",
    "2. Debe leer el documento guía:  \n",
    "  [**gcp_infra_README.md**](../../../workshops/01.block/01.infrastructure/gcp_infra_README.md)\n",
    "3. Descargar `gcloud`\n",
    "  - Verificar configuración: `gcloud config list`\n",
    "  - Configurar proyecto: `gcloud config set project proyecto-master-unir`\n",
    "  - gcloud dataproc clusters list --region=europe-west1\n",
    "  - gsutil ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb31e9c",
   "metadata": {},
   "source": [
    "### **2.2. Parte 1 – HDFS**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff7e34",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<mark>**Comandos gcloud**\n",
    "\n",
    "| Paso | Contexto                | Comando                                                                                          | Comentario                                                                                 |\n",
    "|------|-------------------------|--------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|\n",
    "| 1    | Ver proyecto activo     | `gcloud config list`                                                                             | Confirmar que `project = proyecto-master-unir` y región/zona por defecto son correctas.    |\n",
    "| 2    | Ver buckets GCS         | `gsutil ls`                                                                                      | Listar todos los buckets del proyecto.                                                    |\n",
    "| 3    | Ver carpeta `datos`     | `gsutil ls gs://TU_BUCKET/datos`                                                                 | Confirmar que `flights.csv` está en `gs://TU_BUCKET/datos/flights.csv`.                   |\n",
    "| 4    | Ver clústeres Dataproc  | `gcloud dataproc clusters list --region=europe-west1`                                            | Obtener el `NAME` del clúster (ej: `unircluster`).                                        |\n",
    "| 5    | Conectar al master      | `gcloud compute ssh NOMBRE_CLUSTER-m --zone=europe-west1-d`                                      | Entrar por SSH al nodo master del clúster Dataproc.                                       |\n",
    "| 6    | Comprobar HDFS raíz     | `hdfs dfs -ls /`                                                                                 | Ver contenido del directorio raíz de HDFS.                                                |\n",
    "| 7    | Crear carpeta en HDFS   | `hdfs dfs -mkdir /nombre_apellidos`                                                              | Crear carpeta propia, p. ej. `/alejandro_granados`.                                       |\n",
    "| 8    | Ver carpeta creada      | `hdfs dfs -ls /`                                                                                 | Confirmar que `/nombre_apellidos` existe.                                                 |\n",
    "| 9    | Copiar desde GCS a HDFS | `hdfs dfs -cp gs://TU_BUCKET/datos/flights.csv /nombre_apellidos/flights.csv`                   | Copiar `flights.csv` desde GCS a tu carpeta en HDFS.                                      |\n",
    "| 10   | Ver fichero en HDFS     | `hdfs dfs -ls -h /nombre_apellidos`                                                              | Confirmar que `flights.csv` está en tu carpeta y ver tamaño.                              |\n",
    "| 11   | Ver metadatos básicos   | `hdfs dfs -ls -h /nombre_apellidos/flights.csv`                                                  | Listado detallado: tamaño, permisos, replicación, etc.                                    |\n",
    "| 12   | Ver bloques y ubicación | `hdfs fsck /nombre_apellidos/flights.csv -files -blocks -locations`                              | Ver cómo está distribuido el fichero en HDFS (bloques y datanodes).                       |\n",
    "| 13   | (Alternativa copia)     | `gsutil cp gs://TU_BUCKET/datos/flights.csv .`                                                   | Alternativa: copiar primero del bucket al master (directorio local).                      |\n",
    "| 14   | (Alternativa copia)     | `hdfs dfs -put flights.csv /nombre_apellidos/`                                                   | Luego subir el fichero local al HDFS (en lugar de usar `hdfs dfs -cp gs://...`).          |\n",
    "| 15   | Salir del master        | `exit`                                                                                           | Salir de la sesión SSH del nodo master y volver a tu WSL local.                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618fa994",
   "metadata": {},
   "source": [
    "**1. Crear directorio en HDFS**\n",
    "\n",
    "```bash\n",
    "# Listar proyectos\n",
    "gcloud config list\n",
    "\n",
    "# listar, (validar la zona)\n",
    "gcloud dataproc clusters list --region=europe-west1\n",
    "\n",
    "# conectar en master\n",
    "gcloud compute ssh unircluster-m --zone=europe-west1-b\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Crear objetos\n",
    "hdfs dfs -mkdir /alejandro_xxx\n",
    "hdfs dfs -ls /\n",
    "\n",
    "hdfs dfs -rmdir /alejandro_xxx\n",
    "hdfs dfs -rm -r -f /alejandro_xxx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a662e67e",
   "metadata": {},
   "source": [
    "**2. Subir flights.csv a Cloud Storage (Bucket GCP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1c65b",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Verificar el bucket\n",
    "gsutil ls\n",
    "\n",
    "# Subir flights.csv a la carpeta datos del bucket\n",
    "gsutil cp ../data/flights.csv gs://bucket-unir-hagr27/datos/\n",
    "\n",
    "# enlistar\n",
    "gsutil ls -lh gs://bucket-unir-hagr27/datos/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94891c12",
   "metadata": {},
   "source": [
    "**3. Copiar flights.csv desde GCS al directorio<nombre_usuario> en HDFS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5af944",
   "metadata": {},
   "source": [
    "```bash\n",
    "# copiar dentro de hdfs llamando fichero en gcp \n",
    "hdfs dfs -cp gs://bucket-unir-hagr27/datos/flights.csv /alejandro_xxx/flights.csv\n",
    "\n",
    "# comprobar\n",
    "hdfs dfs -ls -h /alejandro_xxx\n",
    "\n",
    "# Ver metadatos del fichero\n",
    "hdfs dfs -ls -h /alejandro_xxx/flights.csv\n",
    "hdfs fsck /alejandro_xx/flights.csv -files -blocks -locations\n",
    "\n",
    "# Hacer capturas de pantalla de:\n",
    "mkdir, cp/put, ls -h, fsck.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5d9b5",
   "metadata": {},
   "source": [
    "**3. Subir el notebook actividad_1.ipynb al cluster de Dataproc**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d94a403",
   "metadata": {},
   "source": [
    "- Abrir JupyterLab\n",
    "1. En el panel lateral (GCS), subir actividad_1.ipynb.\n",
    "2. Abrir actividad_1.ipynb.\n",
    "3. Arriba a la derecha:\n",
    "    - Cambiar kernel a PySpark si no lo está."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
